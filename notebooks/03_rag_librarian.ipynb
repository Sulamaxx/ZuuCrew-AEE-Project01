{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: \"The Librarian\" (Advanced RAG System)\n",
    "\n",
    "## Project 01 - Operation Ledger-Mind\n",
    "**Course Module:** Weeks 01-03 (Prompt Engineering, Fine-Tuning, Advanced RAG)\n",
    "**Scenario:** Financial Analysis of Uber Technologies (2024 Annual Report)\n",
    "\n",
    "### Technical Requirements Checklist:\n",
    "- [x] **Vector Database**: Weaviate (Cloud or Embedded)\n",
    "- [x] **Hybrid Search**: Dense Vector + BM25 Keyword Search\n",
    "- [x] **Refinement**: Explicit Reciprocal Rank Fusion (RRF)\n",
    "- [x] **Citations**: Exact Page Number Mapping\n",
    "- [x] **Reranking**: Cross-Encoder (ms-marco-MiniLM-L-6-v2)\n",
    "- [x] **Inference**: `query_librarian(question)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup & Dependency Installation\n",
    "\n",
    "Standardizing dependencies for both Google Colab and Local environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def is_colab():\n",
    "    return 'google.colab' in str(get_ipython())\n",
    "\n",
    "if is_colab():\n",
    "    print(\" Detected Google Colab environment.\")\n",
    "    PROJECT_NAME = \"ZuuCrew-AEE-Project01\"\n",
    "    REPO_URL = \"https://github.com/Sulamaxx/ZuuCrew-AEE-Project01.git\"\n",
    "    \n",
    "    if not os.path.exists(PROJECT_NAME):\n",
    "        !git clone {REPO_URL}\n",
    "    else:\n",
    "        !git -C {PROJECT_NAME} pull\n",
    "    \n",
    "    os.chdir(PROJECT_NAME)\n",
    "    \n",
    "    if os.path.abspath(\"src\") not in sys.path:\n",
    "        sys.path.append(os.path.abspath(\"src\"))\n",
    "    \n",
    "    print(\" Installing dependencies...\")\n",
    "    !pip install \"numpy>=1.26.4,<2.0\" -q\n",
    "    !pip install -r requirements.txt -q\n",
    "    !pip install \"weaviate-client>=4.5.4\" -q\n",
    "    \n",
    "    print(\" Installation complete.\")\n",
    "else:\n",
    "    print(\" Running in local environment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment & Advanced Ingestion\n",
    "\n",
    "Preserving **Page Numbers** during ingestion to support exact citations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import yaml\n",
    "from dotenv import load_dotenv\n",
    "from src.services.llm_services import load_config, get_llm, get_text_embeddings\n",
    "from src.utils.data_processing import load_pdf_with_pages, chunk_text\n",
    "\n",
    "# Load environment & config\n",
    "load_dotenv(\".env\" if os.path.exists(\".env\") else \"../.env\")\n",
    "config = load_config(\"src/config/config.yaml\" if os.path.exists(\"src/config/config.yaml\") else \"../src/config/config.yaml\")\n",
    "\n",
    "# Ingestion with Metadata\n",
    "pdf_path = config.get(\"pdf_path\", \"data/pdfs/2024-Annual-Report.pdf\")\n",
    "if not os.path.exists(pdf_path): pdf_path = \"../\" + pdf_path\n",
    "\n",
    "print(f\" Loading document with metadata: {pdf_path}...\")\n",
    "pages = load_pdf_with_pages(pdf_path)\n",
    "\n",
    "processed_chunks = []\n",
    "for pg in pages:\n",
    "    pg_chunks = chunk_text(pg['content'], chunk_size=1500, chunk_overlap=200)\n",
    "    for c in pg_chunks:\n",
    "        processed_chunks.append({\"content\": c, \"page\": pg['page_number']})\n",
    "\n",
    "print(f\" Created {len(processed_chunks)} chunks across {len(pages)} pages.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Weaviate Schema & Indexing (v4 API)\n",
    "\n",
    "Registering properties for keyword (BM25) and vector search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "from weaviate.classes.init import Auth\n",
    "import weaviate.classes as wvc\n",
    "from tqdm import tqdm\n",
    "\n",
    "w_url = os.getenv(\"WEAVIATE_URL\") or config.get(\"weaviate_url\")\n",
    "w_key = os.getenv(\"WEAVIATE_API_KEY\")\n",
    "\n",
    "print(f\" Connecting to Weaviate at {w_url}...\")\n",
    "client = weaviate.connect_to_weaviate_cloud(\n",
    "    cluster_url=w_url,\n",
    "    auth_credentials=Auth.api_key(w_key) if w_key else None,\n",
    "    headers={\"X-Google-Vertex-Api-Key\": os.getenv(\"GOOGLE_API_KEY\", \"\")}\n",
    ")\n",
    "\n",
    "collection_name = \"UberLibrarian\"\n",
    "if client.collections.exists(collection_name):\n",
    "    client.collections.delete(collection_name)\n",
    "\n",
    "uber_report = client.collections.create(\n",
    "    name=collection_name,\n",
    "    vectorizer_config=None,\n",
    "    properties=[\n",
    "        wvc.config.Property(name=\"content\", data_type=wvc.config.DataType.TEXT),\n",
    "        wvc.config.Property(name=\"page\", data_type=wvc.config.DataType.INT),\n",
    "    ]\n",
    ")\n",
    "\n",
    "embeddings_model = get_text_embeddings(config)\n",
    "with uber_report.batch.dynamic() as batch:\n",
    "    for i, chunk in enumerate(tqdm(processed_chunks)):\n",
    "        vector = embeddings_model.embed_query(chunk['content'])\n",
    "        batch.add_object(\n",
    "            properties=chunk,\n",
    "            vector=vector\n",
    "        )\n",
    "\n",
    "print(\" Indexing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hybrid RAG Pipeline (Explicit RRF + Cross-Encoder)\n",
    "\n",
    "Implementing the core 'Librarian' logic with Fusion and Reranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "llm = get_llm(config)\n",
    "\n",
    "def query_librarian(question, top_k=15, final_n=4):\n",
    "    # 1. Hybrid Search with EXPLICIT RRF (fusion_type)\n",
    "    query_vector = embeddings_model.embed_query(question)\n",
    "    \n",
    "    response = uber_report.query.hybrid(\n",
    "        query=question,\n",
    "        vector=query_vector,\n",
    "        alpha=0.5,\n",
    "        fusion_type=wvc.query.HybridFusion.RELATIVE_SCORE, # Advanced Rank Fusion\n",
    "        limit=top_k\n",
    "    )\n",
    "    \n",
    "    candidates = [{\"content\": obj.properties[\"content\"], \"page\": obj.properties[\"page\"]} for obj in response.objects]\n",
    "    \n",
    "    # 2. Cross-Encoder Reranking\n",
    "    pairs = [[question, cand['content']] for cand in candidates]\n",
    "    scores = reranker.predict(pairs)\n",
    "    \n",
    "    # Sort and pick top results\n",
    "    sorted_indices = torch.argsort(torch.tensor(scores), descending=True)[:final_n]\n",
    "    reranked = [candidates[i] for i in sorted_indices]\n",
    "    \n",
    "    # 3. LLM Generation with CITATIONS\n",
    "    context_blocks = []\n",
    "    for i, r in enumerate(reranked):\n",
    "        context_blocks.append(f\"[DOC {i+1} | Page {r['page']}]: {r['content']}\")\n",
    "    \n",
    "    context_str = \"\\n\\n\".join(context_blocks)\n",
    "    system_msg = \"You are 'The Librarian'. Answer questions precisely based on the context. You MUST cite the page numbers used (e.g., [Page 45]). If the context doesn't have the answer, say you don't know.\"\n",
    "    \n",
    "    prompt = f\"Context Blocks:\\n{context_str}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "    ans = llm.invoke([(\"system\", system_msg), (\"user\", prompt)])\n",
    "    \n",
    "    return ans.content if hasattr(ans, 'content') else ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Verification\n",
    "\n",
    "Demonstrating precise citations for numbers and entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_queries = [\n",
    "    \"What was Uber's total revenue in 2024?\",\n",
    "    \"What are the specific risk factors mentioned regarding autonomous vehicle competitors?\",\n",
    "    \"How many monthly active platform consumers (MAPCs) did Uber have in Q4 2024?\"\n",
    "]\n",
    "\n",
    "for q in test_queries:\n",
    "    print(f\"\\n{'='*50}\\nQUERY: {q}\\n{'='*50}\")\n",
    "    print(f\"RESPONSE: {query_librarian(q)}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}