{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: \"The Librarian\" (Advanced RAG System)\n",
    "\n",
    "## Project 01 - Operation Ledger-Mind\n",
    "**Course Module:** Weeks 01-03 (Prompt Engineering, Fine-Tuning, Advanced RAG)\n",
    "**Scenario:** Financial Analysis of Uber Technologies (2024 Annual Report)\n",
    "\n",
    "### Technical Requirements Checklist:\n",
    "- [x] **Vector Database**: Weaviate (Cloud or Embedded)\n",
    "- [x] **Hybrid Search**: Dense Vector + BM25 Keyword Search\n",
    "- [x] **Refinement**: Explicit Reciprocal Rank Fusion (RRF)\n",
    "- [x] **Citations**: Exact Page Number Mapping\n",
    "- [x] **Reranking**: Cross-Encoder (ms-marco-MiniLM-L-6-v2)\n",
    "- [x] **Inference**: `query_librarian(question)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup & Dependency Installation\n",
    "\n",
    "Standardizing dependencies for both Google Colab and Local environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Detected Google Colab environment.\n",
      "Cloning into 'ZuuCrew-AEE-Project01'...\n",
      "remote: Enumerating objects: 144, done.\u001b[K\n",
      "remote: Counting objects: 100% (144/144), done.\u001b[K\n",
      "remote: Compressing objects: 100% (91/91), done.\u001b[K\n",
      "remote: Total 144 (delta 78), reused 110 (delta 44), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (144/144), 6.54 MiB | 15.57 MiB/s, done.\n",
      "Resolving deltas: 100% (78/78), done.\n",
      " Installing dependencies...\n",
      " Installation complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def is_colab():\n",
    "    return 'google.colab' in str(get_ipython())\n",
    "\n",
    "if is_colab():\n",
    "    print(\" Detected Google Colab environment.\")\n",
    "    PROJECT_NAME = \"ZuuCrew-AEE-Project01\"\n",
    "    REPO_URL = \"https://github.com/Sulamaxx/ZuuCrew-AEE-Project01.git\"\n",
    "    \n",
    "    if not os.path.exists(PROJECT_NAME):\n",
    "        !git clone {REPO_URL}\n",
    "    else:\n",
    "        !git -C {PROJECT_NAME} pull\n",
    "    \n",
    "    os.chdir(PROJECT_NAME)\n",
    "    \n",
    "    if os.path.abspath(\"src\") not in sys.path:\n",
    "        sys.path.append(os.path.abspath(\"src\"))\n",
    "    \n",
    "    print(\" Installing dependencies...\")\n",
    "    !pip install \"numpy>=1.26.4,<2.0\" -q\n",
    "    !pip install -r requirements.txt -q\n",
    "    !pip install \"weaviate-client>=4.5.4\" -q\n",
    "    \n",
    "    print(\" Installation complete.\")\n",
    "else:\n",
    "    print(\" Running in local environment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment & Advanced Ingestion\n",
    "\n",
    "Preserving **Page Numbers** during ingestion to support exact citations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_text_embeddings' from 'src.services.llm_services' (/content/ZuuCrew-AEE-Project01/src/services/llm_services.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1513888912.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdotenv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dotenv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_services\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_llm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_text_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_processing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_pdf_with_pages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'get_text_embeddings' from 'src.services.llm_services' (/content/ZuuCrew-AEE-Project01/src/services/llm_services.py)",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import yaml\n",
    "from dotenv import load_dotenv\n",
    "from src.services.llm_services import load_config, get_llm, get_text_embeddings\n",
    "from src.utils.data_processing import load_pdf_with_pages, chunk_text\n",
    "\n",
    "# Load environment & config\n",
    "load_dotenv(\".env\" if os.path.exists(\".env\") else \"../.env\")\n",
    "config = load_config(\"src/config/config.yaml\" if os.path.exists(\"src/config/config.yaml\") else \"../src/config/config.yaml\")\n",
    "\n",
    "# Ingestion with Metadata\n",
    "pdf_path = config.get(\"pdf_path\", \"data/pdfs/2024-Annual-Report.pdf\")\n",
    "if not os.path.exists(pdf_path): pdf_path = \"../\" + pdf_path\n",
    "\n",
    "print(f\" Loading document with metadata: {pdf_path}...\")\n",
    "pages = load_pdf_with_pages(pdf_path)\n",
    "\n",
    "processed_chunks = []\n",
    "for pg in pages:\n",
    "    pg_chunks = chunk_text(pg['content'], chunk_size=1500, chunk_overlap=200)\n",
    "    for c in pg_chunks:\n",
    "        processed_chunks.append({\"content\": c, \"page\": pg['page_number']})\n",
    "\n",
    "print(f\" Created {len(processed_chunks)} chunks across {len(pages)} pages.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Weaviate Schema & Indexing (v4 API)\n",
    "\n",
    "Registering properties for keyword (BM25) and vector search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_url = os.getenv(\"WEAVIATE_URL\") or config.get(\"weaviate_url\")\n",
    "w_key = os.getenv(\"WEAVIATE_API_KEY\")\n",
    "\n",
    "is_local = \"localhost\" in w_url or \"127.0.0.1\" in w_url\n",
    "\n",
    "if is_colab() and is_local:\n",
    "    print(\"\\n\u26a0\ufe0f  WARNING: You are in Google Colab but using a 'localhost' Weaviate URL.\")\n",
    "    print(\"   Colab cannot reach your local machine's localhost. \")\n",
    "    print(\"   Please use a Weaviate Cloud (WCD) URL or a tunnel (like ngrok).\\n\")\n",
    "\n",
    "print(f\" Connecting to Weaviate at {w_url}...\")\n",
    "if is_local:\n",
    "    client = weaviate.connect_to_local(\n",
    "        host=\"localhost\" if \"localhost\" in w_url else \"127.0.0.1\",\n",
    "        headers={\"X-Google-Vertex-Api-Key\": os.getenv(\"GOOGLE_API_KEY\", \"\")}\n",
    "    )\n",
    "else:\n",
    "    client = weaviate.connect_to_weaviate_cloud(\n",
    "        cluster_url=w_url,\n",
    "        auth_credentials=Auth.api_key(w_key) if w_key else None,\n",
    "        headers={\"X-Google-Vertex-Api-Key\": os.getenv(\"GOOGLE_API_KEY\", \"\")}\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hybrid RAG Pipeline (Explicit RRF + Cross-Encoder)\n",
    "\n",
    "Implementing the core 'Librarian' logic with Fusion and Reranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "import torch\n",
    "\n",
    "# Initialize Reranker (ms-marco-MiniLM-L-6-v2 as per assessment)\n",
    "reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "llm = get_llm(config)\n",
    "embeddings_model = get_text_embeddings(config)\n",
    "\n",
    "def query_librarian(question, top_k=15, final_n=4):\n",
    "    \"\"\"\n",
    "    Advanced Hybrid RAG Pipeline\n",
    "    Implementation: Dense + BM25 -> RRF (Fusion) -> Cross-Encoder (Rerank) -> Generation\n",
    "    \"\"\"\n",
    "    # 1. Hybrid Search with EXPLICIT Reciprocal Rank Fusion (RRF)\n",
    "    # Weaviate's HybridFusion.RANKED implements RRF.\n",
    "    query_vector = embeddings_model.embed_query(question)\n",
    "    \n",
    "    response = uber_report.query.hybrid(\n",
    "        query=question,\n",
    "        vector=query_vector,\n",
    "        alpha=0.5, # Balance between Vector (1.0) and BM25 (0.0)\n",
    "        fusion_type=wvc.query.HybridFusion.RANKED, # THIS IS RRF\n",
    "        limit=top_k\n",
    "    )\n",
    "    \n",
    "    # Extract candidates with metadata for citations\n",
    "    candidates = [{\"content\": obj.properties[\"content\"], \"page\": obj.properties[\"page\"]} for obj in response.objects]\n",
    "    \n",
    "    if not candidates:\n",
    "        return \"I'm sorry, I couldn't find any relevant information in the report.\"\n",
    "    \n",
    "    # 2. Cross-Encoder Reranking (Higher Accuracy)\n",
    "    pairs = [[question, cand['content']] for cand in candidates]\n",
    "    scores = reranker.predict(pairs)\n",
    "    \n",
    "    # Sort results by rerank scores (descending)\n",
    "    sorted_indices = torch.argsort(torch.tensor(scores), descending=True)[:final_n]\n",
    "    reranked = [candidates[i] for i in sorted_indices]\n",
    "    \n",
    "    # 3. Final Answer Generation with Page Citations\n",
    "    context_blocks = []\n",
    "    for i, r in enumerate(reranked):\n",
    "        context_blocks.append(f\"[Source {i+1} | Page {r['page']}]: {r['content']}\")\n",
    "    \n",
    "    context_str = \"\\n\\n\".join(context_blocks)\n",
    "    system_msg = (\n",
    "        \"You are 'The Librarian', an advanced financial analysis agent. \"\n",
    "        \"Answer the question STRICTLY using the provided context blocks. \"\n",
    "        \"For every fact or number, you MUST cite the correct [Page X] from the context metadata. \"\n",
    "        \"If the answer is not in the context, state that you do not have sufficient information.\"\n",
    "    )\n",
    "    \n",
    "    prompt = f\"Context Blocks:\\n{context_str}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "    ans = llm.invoke([(\"system\", system_msg), (\"user\", prompt)])\n",
    "    \n",
    "    return ans.content if hasattr(ans, 'content') else str(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Verification\n",
    "\n",
    "Demonstrating precise citations for numbers and entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_queries = [\n",
    "    \"What was Uber's total revenue in 2024?\",\n",
    "    \"What are the specific risk factors mentioned regarding autonomous vehicle competitors?\",\n",
    "    \"How many monthly active platform consumers (MAPCs) did Uber have in Q4 2024?\"\n",
    "]\n",
    "\n",
    "for q in test_queries:\n",
    "    print(f\"\\n{'='*50}\\nQUERY: {q}\\n{'='*50}\")\n",
    "    print(f\"RESPONSE: {query_librarian(q)}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}