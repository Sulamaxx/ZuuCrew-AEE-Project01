{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: \"The Showdown\" (Comprehensive Evaluation)\n",
    "\n",
    "## Project 01 - Operation Ledger-Mind\n",
    "**Course Module:** Weeks 01-03 (Prompt Engineering, Fine-Tuning, Advanced RAG)\n",
    "**Scenario:** Head-to-Head Comparison of LLM Architectures\n",
    "\n",
    "### Evaluation Metrics:\n",
    "- **Lexical**: ROUGE-L (Overlap with Ground Truth)\n",
    "- **LLM-as-a-Judge**: Accuracy (1-5) and Faithfulness (1-5)\n",
    "- **Performance**: Latency (Seconds per Response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup & Integration\n",
    "\n",
    "Standardizing environment and ensuring all model query functions are accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from rouge_score import rouge_scorer\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def is_colab():\n",
    "    return 'google.colab' in str(get_ipython())\n",
    "\n",
    "if is_colab():\n",
    "    PROJECT_NAME = \"ZuuCrew-AEE-Project01\"\n",
    "    if not os.path.exists(PROJECT_NAME): \n",
    "        !git clone https://github.com/Sulamaxx/ZuuCrew-AEE-Project01.git\n",
    "    os.chdir(PROJECT_NAME)\n",
    "    if os.path.abspath(\"src\") not in sys.path: sys.path.append(os.path.abspath(\"src\"))\n",
    "\n",
    "load_dotenv()\n",
    "from src.services.llm_services import load_config, get_llm\n",
    "from src.utils.json_helper import extract_json_from_llm\n",
    "\n",
    "config = load_config(\"src/config/config.yaml\")\n",
    "llm = get_llm(config)\n",
    "\n",
    "print(\" Setup complete. Ready for evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Metric Functions\n",
    "\n",
    "Implementing ROUGE-L and an automated LLM-as-a-Judge system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rouge_l(prediction, reference):\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference, prediction)\n",
    "    return scores['rougeL'].fmeasure\n",
    "\n",
    "def llm_judge(question, prediction, ground_truth):\n",
    "    prompt = f\"\"\"Evaluate the following answer against a ground truth answer for a financial analysis query.\n",
    "Question: {question}\n",
    "Prediction: {prediction}\n",
    "Ground Truth: {ground_truth}\n",
    "\n",
    "Score from 1-5 for:\n",
    "1. Accuracy (Is the information correct and detailed?)\n",
    "2. Faithfulness (Is the answer supported by the technical data provided in ground truth?)\n",
    "\n",
    "Return ONLY a JSON object: {{\"accuracy\": X, \"faithfulness\": Y, \"reasoning\": \"...\"}}\"\"\"\n",
    "    \n",
    "    response = llm.invoke([(\"system\", \"You are a Senior Technical Auditor.\"), (\"user\", prompt)])\n",
    "    return extract_json_from_llm(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Wrappers\n",
    "\n",
    "Defining common interfaces for the Base Model, The Intern, and The Librarian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_base(question):\n",
    "    \"\"\"Zero-shot query to the base Gemini model without any context.\"\"\"\n",
    "    res = llm.invoke(f\"Base on your general knowledge, answer: {question}\")\n",
    "    return res.content if hasattr(res, 'content') else res\n",
    "\n",
    "def query_intern_placeholder(question):\n",
    "    \"\"\"Placeholder for the Fine-Tuned Intern model.\"\"\"\n",
    "    # In a real environment, you would load the LoRA model here.\n",
    "    return \"[Fine-Tuned Response Placeholder]\"\n",
    "\n",
    "def query_librarian_placeholder(question):\n",
    "    \"\"\"Placeholder for the Advanced RAG Librarian.\"\"\"\n",
    "    # In a real environment, you would call the Librarian from Notebook 03.\n",
    "    return \"[RAG Response Placeholder]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Grand Showdown\n",
    "\n",
    "Running the evaluation loop on the competitive test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_path = config.get(\"train_data_path\", \"data/generated/golden_test_set.jsonl\")\n",
    "if not os.path.exists(test_set_path): test_set_path = \"../\" + test_set_path\n",
    "\n",
    "eval_data = []\n",
    "with open(test_set_path, 'r') as f:\n",
    "    for line in f: eval_data.append(json.loads(line))\n",
    "\n",
    "# For demonstration, we'll evaluate 5 samples\n",
    "samples = eval_data[:5]\n",
    "results = []\n",
    "\n",
    "print(f\" Starting showdown on {len(samples)} samples...\")\n",
    "\n",
    "for item in tqdm(samples):\n",
    "    q, gt = item['question'], item['answer']\n",
    "    \n",
    "    for model_name, query_fn in [\n",
    "        (\"Base\", query_base), \n",
    "        (\"Intern\", query_intern_placeholder), \n",
    "        (\"Librarian\", query_librarian_placeholder)\n",
    "    ]:\n",
    "        start_time = time.time()\n",
    "        pred = query_fn(q)\n",
    "        latency = time.time() - start_time\n",
    "        \n",
    "        rouge = calculate_rouge_l(pred, gt)\n",
    "        # Note: In a real run, only judge non-placeholder responses\n",
    "        judge = llm_judge(q, pred, gt) if \"Placeholder\" not in pred else {\"accuracy\": 0, \"faithfulness\": 0}\n",
    "        \n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Question\": q,\n",
    "            \"ROUGE-L\": rouge,\n",
    "            \"Accuracy\": judge.get('accuracy', 0),\n",
    "            \"Faithfulness\": judge.get('faithfulness', 0),\n",
    "            \"Latency (s)\": latency\n",
    "        })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "print(\"\\n Showdown complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Final Comparison Table\n",
    "\n",
    "Aggregating results to see which architecture is superior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = df_results.groupby(\"Model\")[[\"ROUGE-L\", \"Accuracy\", \"Faithfulness\", \"Latency (s)\"]].mean().reset_index()\n",
    "display(summary.sort_values(\"Accuracy\", ascending=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}