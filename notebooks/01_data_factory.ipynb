{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: The Data Factory\n",
    "\n",
    "This notebook implements the data generation pipeline to transform the `2024-Annual-Report.pdf` into a fine-tuning dataset. \n",
    "\n",
    "**Assessment Requirement:** Generate 10 Q/A pairs for **each** chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "# Add project src to path\n",
    "sys.path.append(os.path.abspath(\"../\"))\n",
    "\n",
    "from src.services.llm_services import load_config, get_llm\n",
    "from src.utils.data_processing import load_and_clean_pdf, chunk_text\n",
    "from src.utils.json_helper import extract_json_from_llm\n",
    "\n",
    "config = load_config(\"../src/config/config.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Ingestion & Cleaning\n",
    "\n",
    "**Step 1:** Load the documents and clean them (remove headers, footers, and extra whitespace)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded and cleaned 632153 characters.\n"
     ]
    }
   ],
   "source": [
    "pdf_path = os.path.join(\"..\", config.get(\"pdf_path\", \"data/pdfs/2024-Annual-Report.pdf\"))\n",
    "raw_text = load_and_clean_pdf(pdf_path)\n",
    "print(f\"Loaded and cleaned {len(raw_text)} characters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chunking Strategy\n",
    "\n",
    "**Step 2:** Split the documents into chunks of 1500 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 487 chunks.\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 1500\n",
    "chunk_overlap = 200\n",
    "chunks = chunk_text(raw_text, chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "print(f\"Created {len(chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Generation Loop\n",
    "\n",
    "**Step 3:** For each chunk, generate 10 Q/A pairs using a two-step pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step 3: Processing all chunks: 100%|██████████| 487/487 [2:26:31<00:00, 18.05s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Generated Q/A Pairs: 4710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "llm = get_llm(config)\n",
    "\n",
    "QUESTION_GEN_SYSTEM = \"\"\"You are a Financial Data Architect. Generate 10 distinct questions based ONLY on the provided chunk.\n",
    "Balance across: HARD FACTS, STRATEGIC SUMMARIES, and STYLISTIC/CREATIVE outputs.\n",
    "Return ONLY a JSON list of strings [\\\"Q1\\\", \\\"Q2\\\", ...].\"\"\"\n",
    "\n",
    "ANSWER_GEN_SYSTEM = \"\"\"You are a Senior Financial Analyst. Answer the provided questions based strictly on the context.\n",
    "Return ONLY a JSON list of objects with 'question' and 'answer' keys.\"\"\"\n",
    "\n",
    "dataset = []\n",
    "\n",
    "# ASSESSMENT REQUIREMENT: GENERATE FOR ALL CHUNKS\n",
    "target_chunks = chunks\n",
    "\n",
    "for i, chunk in enumerate(tqdm(target_chunks, desc=\"Step 3: Processing all chunks\")):\n",
    "    try:\n",
    "        # Step A: Questions\n",
    "        q_res = llm.invoke([(\"system\", QUESTION_GEN_SYSTEM), (\"user\", f\"Chunk Content: {chunk}\")])\n",
    "        questions = extract_json_from_llm(q_res.content)\n",
    "        if not questions or not isinstance(questions, list): continue\n",
    "        \n",
    "        # Step B: Answers\n",
    "        a_res = llm.invoke([(\"system\", ANSWER_GEN_SYSTEM), (\"user\", f\"Context: {chunk}\\n\\nQuestions: {json.dumps(questions)}\")])\n",
    "        pairs = extract_json_from_llm(a_res.content)\n",
    "        if pairs and isinstance(pairs, list): dataset.extend(pairs)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in chunk {i}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\nTotal Generated Q/A Pairs: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Storage & Splitting\n",
    "\n",
    "**Step 4:** Store the generated data in JSONL format and split 80/20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 3768 to train.jsonl and 942 to golden_test_set.jsonl\n"
     ]
    }
   ],
   "source": [
    "if dataset:\n",
    "    random.shuffle(dataset)\n",
    "    split_idx = int(0.8 * len(dataset))\n",
    "    train_set, test_set = dataset[:split_idx], dataset[split_idx:]\n",
    "    \n",
    "    out_dir = os.path.join(\"..\", config['train_data_path'])\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    \n",
    "    with open(os.path.join(out_dir, 'train.jsonl'), 'w') as f:\n",
    "        for item in train_set: f.write(json.dumps(item) + \"\\n\")\n",
    "    with open(os.path.join(out_dir, 'golden_test_set.jsonl'), 'w') as f:\n",
    "        for item in test_set: f.write(json.dumps(item) + \"\\n\")\n",
    "            \n",
    "    print(f\"Saved {len(train_set)} to train.jsonl and {len(test_set)} to golden_test_set.jsonl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
