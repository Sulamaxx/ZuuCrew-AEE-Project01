{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: \"The Intern\" (Fine-Tuning)\n",
    "\n",
    "## Project 01 - Operation Ledger-Mind\n",
    "**Course Module:** Weeks 01-03 (Prompt Engineering, Fine-Tuning, Advanced RAG)\n",
    "**Scenario:** Financial Analysis of Uber Technologies (2024 Annual Report)\n",
    "\n",
    "###  Technical Requirements Checklist:\n",
    "- [x] **Hugging Face Ecosystem**: transformers, peft, trl, bitsandbytes\n",
    "- [x] **Base Model**: Qwen/Qwen2.5-1.5B-Instruct (Optimized for T4)\n",
    "- [x] **Quantization**: 4-bit NF4 with double quantization\n",
    "- [x] **Adapter Config**: LoRA (Targets: q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj)\n",
    "- [x] **Training**: SFTTrainer for 100 steps\n",
    "- [x] **Inference**: `query_intern(question)`\n",
    "- [x] **Evaluation**: local ROUGE-L baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup & Dependency Installation\n",
    "\n",
    "Standardizing dependencies for both Google Colab and Local environments. Note: `numpy==1.26.4` is required for `trl==0.9.6` compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Detected Google Colab environment.\n",
      " Repository already exists. Fetching latest updates...\n",
      "remote: Enumerating objects: 7, done.\u001b[K\n",
      "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
      "remote: Compressing objects: 100% (1/1), done.\u001b[K\n",
      "remote: Total 4 (delta 3), reused 4 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
      "Unpacking objects: 100% (4/4), 2.23 KiB | 1.12 MiB/s, done.\n",
      "From https://github.com/Sulamaxx/ZuuCrew-AEE-Project01\n",
      "   8952d10..fe570f4  main       -> origin/main\n",
      "Updating 8952d10..fe570f4\n",
      "Fast-forward\n",
      " fix_keyerror.py                      | 29 \u001b[31m-------------------\u001b[m\n",
      " notebooks/02_finetuning_intern.ipynb | 55 \u001b[32m+++++++++++++++++\u001b[m\u001b[31m-------------------\u001b[m\n",
      " 2 files changed, 26 insertions(+), 58 deletions(-)\n",
      " delete mode 100644 fix_keyerror.py\n",
      " Installing dependencies from requirements.txt...\n",
      " Installation complete. Please Restart Session if you see numpy binary incompatibility errors.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def is_colab():\n",
    "    return 'google.colab' in str(get_ipython())\n",
    "\n",
    "if is_colab():\n",
    "    print(\" Detected Google Colab environment.\")\n",
    "    PROJECT_NAME = \"ZuuCrew-AEE-Project01\"\n",
    "    REPO_URL = \"https://github.com/Sulamaxx/ZuuCrew-AEE-Project01.git\"\n",
    "    \n",
    "    if not os.path.exists(PROJECT_NAME):\n",
    "        print(f\" Cloning repository from {REPO_URL}...\")\n",
    "        !git clone {REPO_URL}\n",
    "    else:\n",
    "        print(f\" Repository already exists. Fetching latest updates...\")\n",
    "        !git -C {PROJECT_NAME} pull\n",
    "    \n",
    "    if os.getcwd().split('/')[-1] != PROJECT_NAME:\n",
    "        os.chdir(PROJECT_NAME)\n",
    "    \n",
    "    # Standardize src path\n",
    "    if os.path.abspath(\"src\") not in sys.path:\n",
    "        sys.path.append(os.path.abspath(\"src\"))\n",
    "    \n",
    "    print(\" Installing dependencies from requirements.txt...\")\n",
    "    # Install specific numpy range first to prevent binary incompatibility with transformers/trl\n",
    "    !pip install \"numpy>=1.26.4,<2.0\" -q\n",
    "    !pip install -r requirements.txt -q\n",
    "    \n",
    "    print(\" Installation complete. Please Restart Session if you see numpy binary incompatibility errors.\")\n",
    "else:\n",
    "    print(\" Running in local environment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Diagnostics & Configuration\n",
    "\n",
    "Verifying hardware compatibility and loading the centralized configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ENVIRONMENT & GPU CHECK\n",
      "============================================================\n",
      "PyTorch: 2.9.0+cu128\n",
      "Device: Tesla T4\n",
      "VRAM: 14.56 GB\n",
      "Compute Dtype: torch.float16\n",
      "BF16 Support: False\n",
      "============================================================\n",
      "{'artifacts_root': './artifacts',\n",
      " 'base_model': 'Qwen/Qwen2.5-1.5B-Instruct',\n",
      " 'chunk_overlap': 200,\n",
      " 'chunk_size': 1500,\n",
      " 'data_root': './data',\n",
      " 'gradient_accumulation_steps': 64,\n",
      " 'hybrid_search_alpha': 0.5,\n",
      " 'learning_rate': '2e-5',\n",
      " 'llm_model': 'gemini-2.5-flash',\n",
      " 'llm_provider': 'gemini',\n",
      " 'lora_alpha': 32,\n",
      " 'lora_dropout': 0.05,\n",
      " 'lora_r': 16,\n",
      " 'max_steps': 100,\n",
      " 'max_tokens': 4096,\n",
      " 'normalize_embeddings': True,\n",
      " 'pdf_path': './data/pdfs/2024-Annual-Report.pdf',\n",
      " 'per_device_train_batch_size': 1,\n",
      " 'rerank_top_n': 3,\n",
      " 'similarity_top_k': 5,\n",
      " 'temperature': 0.2,\n",
      " 'text_emb_model': 'sentence-transformers/all-MiniLM-L6-v2',\n",
      " 'text_emb_provider': 'sbert',\n",
      " 'train_data_path': './artifacts/train_data',\n",
      " 'weaviate_url': 'http://localhost:8080'}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import yaml\n",
    "import random\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from pprint import pprint\n",
    "\n",
    "# 1. Load Environment & Config\n",
    "load_dotenv(\".env\" if os.path.exists(\".env\") else \"../.env\")\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "config_path = \"src/config/config.yaml\" if os.path.exists(\"src/config/config.yaml\") else \"../src/config/config.yaml\"\n",
    "with open(config_path, \"r\") as f:\n",
    "    CONFIG_YAML = yaml.safe_load(f)\n",
    "\n",
    "# 2. Seed for Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# 3. Hardware Diagnostics (from Workshop)\n",
    "use_bf16 = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8\n",
    "compute_dtype = torch.bfloat16 if use_bf16 else torch.float16\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ENVIRONMENT & GPU CHECK\")\n",
    "print(\"=\"*60)\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    print(f\"Compute Dtype: {compute_dtype}\")\n",
    "    print(f\"BF16 Support: {use_bf16}\")\n",
    "else:\n",
    "    print(\" WARNING: No CUDA GPU detected. Training will fail.\")\n",
    "print(\"=\"*60)\n",
    "pprint(CONFIG_YAML)\n",
    "\n",
    "# Normalize types for SFTTrainer (prevents TypeErrors from YAML strings)\n",
    "for key in ['learning_rate', 'lora_alpha', 'lora_dropout', 'temperature']:\n",
    "    if key in CONFIG_YAML: CONFIG_YAML[key] = float(CONFIG_YAML[key])\n",
    "for key in ['lora_r', 'max_steps', 'per_device_train_batch_size', 'gradient_accumulation_steps']:\n",
    "    if key in CONFIG_YAML: CONFIG_YAML[key] = int(CONFIG_YAML[key])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model & Quantization Implementation\n",
    "\n",
    "Implementing 4-bit NF4 quantization with double quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading model: Qwen/Qwen2.5-1.5B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = CONFIG_YAML.get(\"base_model\", \"Qwen/Qwen2.5-1.5B-Instruct\")\n",
    "\n",
    "# 4-bit Quantization Config (Standardized for T4 and RTX)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype\n",
    ")\n",
    "\n",
    "print(f\" Loading model: {base_model_id}...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    token=hf_token\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id, token=hf_token)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LoRA Configuration (Expanded Adapters)\n",
    "\n",
    "Injecting trainable Rank-Adaptive matrices. We expand the target modules to include MLP layers for better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 18,464,768 || all params: 1,562,179,072 || trainable%: 1.1820\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=CONFIG_YAML.get(\"lora_r\", 16),\n",
    "    lora_alpha=CONFIG_YAML.get(\"lora_alpha\", 32),\n",
    "    lora_dropout=CONFIG_YAML.get(\"lora_dropout\", 0.05),\n",
    "    # Expanded targets modules from Workshop for better coverage\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset Loading & ChatML Formatting\n",
    "\n",
    "Formatting the generated Uber instruction data into ChatML structure with proper validation splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 3768 training examples.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_path = \"artifacts/train_data/train.jsonl\"\n",
    "if not os.path.exists(train_path):\n",
    "    train_path = \"../\" + train_path\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=train_path, split=\"train\")\n",
    "\n",
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['question'])):\n",
    "        # ChatML Structure\n",
    "        text = f\"<|im_start|>system\\nYou are a professional financial analyst assistant. Answer questions based on Uber's 2024 Annual Report.<|im_end|>\\n<|im_start|>user\\n{example['question'][i]}<|im_end|>\\n<|im_start|>assistant\\n{example['answer'][i]}<|im_end|>\"\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "print(f\" Loaded {len(dataset)} training examples.\")\n",
    "\n",
    "# Split for evaluation during training\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=SEED)\n",
    "train_data = dataset[\"train\"]\n",
    "val_data = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Execution\n",
    "\n",
    "Executing the SFT loop. We use `SFTConfig` for better integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 1:00:39, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.060800</td>\n",
       "      <td>1.593384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.341900</td>\n",
       "      <td>1.366967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.316300</td>\n",
       "      <td>1.308827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.297200</td>\n",
       "      <td>1.294674</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training Complete. Adapters saved to artifacts/intern_final_adapter\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"artifacts/intern_checkpoints\",\n",
    "    per_device_train_batch_size=CONFIG_YAML.get(\"per_device_train_batch_size\", 1),\n",
    "    gradient_accumulation_steps=CONFIG_YAML.get(\"gradient_accumulation_steps\", 64),\n",
    "    learning_rate=CONFIG_YAML.get(\"learning_rate\", 2e-5),\n",
    "    logging_steps=10,\n",
    "    max_steps=CONFIG_YAML.get(\"max_steps\", 100),\n",
    "    save_steps=50,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    fp16=not use_bf16,\n",
    "    bf16=use_bf16,\n",
    "    max_seq_length=1024,\n",
    "    packing=False,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=25,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    peft_config=peft_config,\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    args=training_args\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"artifacts/intern_final_adapter\")\n",
    "print(\" Training Complete. Adapters saved to artifacts/intern_final_adapter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Inference Pipeline: `query_intern` \n",
    "\n",
    "Critical inference function using ChatML prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What were the key drivers of Uber's revenue growth in 2024?\n",
      "A: The key drivers of Uber's revenue growth in 2024 included an increase in average ride and car-hailing per-ride prices, higher volumes of rides and trips, and increased usage of services such as food delivery, grocery pickup, and other third-party apps integrated into the platform.\n"
     ]
    }
   ],
   "source": [
    "def query_intern(question, max_new_tokens=256):\n",
    "    prompt = f\"<|im_start|>system\\nYou are a professional financial analyst assistant. Answer questions based on Uber's 2024 Annual Report.<|im_end|>\\n<|im_start|>user\\n{question}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=max_new_tokens, \n",
    "            temperature=0.1, \n",
    "            do_sample=True, \n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    if \"assistant\" in response:\n",
    "        return response.split(\"assistant\")[-1].strip()\n",
    "    return response.strip()\n",
    "\n",
    "# Sample test\n",
    "test_q = \"What were the key drivers of Uber's revenue growth in 2024?\"\n",
    "print(f\"Q: {test_q}\\nA: {query_intern(test_q)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Local Evaluation (ROUGE-L)\n",
    "\n",
    "Testing performance on the Golden Test Set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fbd7350f751448885185e7a445fd2cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Evaluating samples from Golden Test Set...\n",
      "--- Sample 1 ---\n",
      "Q: What external factor is highlighted as making system and control improvements particularly challenging?\n",
      "ROUGE-L: 0.4528\n",
      "--- Sample 2 ---\n",
      "Q: Beyond regulatory compliance, what broader adverse effects could the company face if it cannot offer autonomous vehicle technologies in the manner it expects?\n",
      "ROUGE-L: 0.1429\n",
      "--- Sample 3 ---\n",
      "Q: What specific financial statement components, beyond just 'costs,' are explicitly mentioned as potentially impacted by the reclassification of Drivers?\n",
      "ROUGE-L: 0.1111\n",
      "--- Sample 4 ---\n",
      "Q: Besides the specific material legal proceedings, what general category of litigation is explicitly excluded from the \"material pending legal proceedings\" discussed?\n",
      "ROUGE-L: 0.4348\n",
      "--- Sample 5 ---\n",
      "Q: What distinct regulatory considerations apply to the company's Delivery and Freight products, as opposed to its Mobility business?\n",
      "ROUGE-L: 0.3137\n",
      "\n",
      " Average ROUGE-L Baseline: 0.2911\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "test_path = \"artifacts/train_data/golden_test_set.jsonl\"\n",
    "if not os.path.exists(test_path):\n",
    "    test_path = \"../\" + test_path\n",
    "\n",
    "test_dataset = load_dataset(\"json\", data_files=test_path, split=\"train\")\n",
    "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "scores = []\n",
    "\n",
    "print(f\" Evaluating samples from Golden Test Set...\")\n",
    "\n",
    "for i in range(min(5, len(test_dataset))):\n",
    "    question = test_dataset[i]['question']\n",
    "    ground_truth = test_dataset[i]['answer']\n",
    "    prediction = query_intern(question)\n",
    "    \n",
    "    score = scorer.score(ground_truth, prediction)['rougeL'].fmeasure\n",
    "    scores.append(score)\n",
    "    \n",
    "    print(f\"--- Sample {i+1} ---\")\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"ROUGE-L: {score:.4f}\")\n",
    "\n",
    "print(f\"\\n Average ROUGE-L Baseline: {np.mean(scores):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
